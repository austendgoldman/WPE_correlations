{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import warnings\n",
    "import scipy.spatial\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, Normalizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "sns.set(color_codes='Blue', style=\"whitegrid\")\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "sns.set_context(rc={'patch.linewidth': 0.0})\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Objective:\n",
    "# Create a merged dataset of testing and mapping data and then determine which features are correlated to the target variable 'WPE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main functions\n",
    "\n",
    "def corr_results(df):\n",
    "    \"\"\"\n",
    "    Iterate over all columns in dataframe and find column pairs that have significant correlations.\n",
    "    Ignore column pairs that have the same str in name (i.e. \"Power_1\" & \"Power_2\")\n",
    "    :param df: Dataframe object\n",
    "    :return: Pairs of columns that have significant correlations.\n",
    "    \"\"\"\n",
    "    corr_dict = {}\n",
    "    for c1 in df.columns[0:-1]:\n",
    "        for c2 in df.loc[:, c1:].columns:\n",
    "            c1_str = (c1.split('_'))[0]\n",
    "            c2_str = (c2.split('_'))[0]\n",
    "            if c2 != c1:\n",
    "                if c1_str != c2_str:\n",
    "                    if c1.split('(')[0] == \"Voltage\" and c2_str in (\"Power\", \"WPE\", \"WP\") or \\\n",
    "                        c1.split('(')[0] == \"Voltage\" and c2.split('(')[0] == \"Current\" or \\\n",
    "                            c1.split('(')[0] == \"Current\" and c2_str in (\"Power\", \"WPE\", \"WP\", \"OP\", \"Emitt\", \"Emitt:OP\") or \\\n",
    "                            (c1.split('(')[0] == \"Current\" and c2.split('(')[0] == \"Current\") or \\\n",
    "                            (c1.split('(')[0] == \"Current\" and c2_str in (\"OP\", \"Emitt\", \"Emitt:OP\")) or \\\n",
    "                            (c1_str == \"Power\" and c2_str in (\"WPE\", \"WP\")) or \\\n",
    "                            (c1_str == \"WPE\" and c2_str == \"WP\") or \\\n",
    "                            (c1_str == \"Emitt:OP\" and c2_str in (\"OP\", \"Emitt\")) or (c1_str == \"Emitt:OP\" and c2.split('(')[0] == \"Current\"):\n",
    "                        # print(c1, c2)\n",
    "                        pass\n",
    "                    else:\n",
    "                        corr = df.loc[:, c1].corr(df.loc[:, c2], method='spearman')\n",
    "                        if ((corr >= .5) | (corr <= -.5)) & (corr != 1.000):\n",
    "                            corr_dict[c1 + ':' + c2] = corr\n",
    "                            # print(c1_str, c2_str)\n",
    "                    # print('Correlation:', c1, c2, '=', corr)\n",
    "    results = pd.DataFrame(corr_dict.items(), columns=['Columns', 'Correlation']).sort_values(by='Correlation',ignore_index=True)\n",
    "    return results\n",
    "\n",
    "def corr_target(df, col_name):\n",
    "    \"\"\"\n",
    "    Calculates correlation between the desired feature and all df columns\n",
    "    \"\"\"\n",
    "    feature_target_corr = {}\n",
    "    for col in df:\n",
    "        if col_name != col:\n",
    "            corr, p_val = kendalltau(df[col], df[col_name])\n",
    "            if ((corr >= .2) | (corr <= -.2)) & (corr != 1.000):\n",
    "                feature_target_corr[col + ':' + col_name] = corr\n",
    "    results = pd.DataFrame(feature_target_corr.items(), columns=['Columns', 'Correlation']).sort_values(\n",
    "        by='Correlation', ignore_index=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Iterate over all columns in dataframe and find the columns that have duplicate contents.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    \"\"\"\n",
    "    duplicate_col_names = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x & y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicate_col_names.add(df.columns.values[y])\n",
    "    return list(duplicate_col_names)\n",
    "\n",
    "\n",
    "def single_feature_plot(df, feature):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe object, feature: numerical column\n",
    "    :return: Boxplot and distplot of single numerical feature\n",
    "    \"\"\"\n",
    "    sns.set(color_codes='Blue', style=\"whitegrid\")\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "    sns.set_context(rc={'patch.linewidth': 0.0})\n",
    "    f, (ax1, ax2) = plt.subplots(2, gridspec_kw={\"height_ratios\": (.15, .65)})\n",
    "    filtered = df.loc[~np.isnan(df[feature]), feature]\n",
    "    sns.boxplot(filtered, color='steelblue', ax=ax1)\n",
    "    sns.distplot(filtered, kde=True, hist=True, kde_kws={'linewidth': 1}, color='steelblue', ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def multiple_features_plots(df, plot_type, numerical_cols=None):\n",
    "    \"\"\"\n",
    "    :param: df: Dataframe object\n",
    "    :param: plot_type: boxplot or histogram\n",
    "    :param: numerical_cols: list of features to plot. if None, all numerical features in data frame will be plotted.\n",
    "    :return: Boxplot or histogram of all numerical features in df (unless numerical_cols is specified)\n",
    "    \"\"\"\n",
    "    if numerical_cols is None:\n",
    "        cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    else:\n",
    "        cols = numerical_cols\n",
    "    num_plots = len(cols)\n",
    "    num_cols = math.ceil(np.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "    fig = plt.figure(figsize=(24, 15))\n",
    "    for i in range(len(cols)):\n",
    "        var = cols[i]\n",
    "        sub = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "        sub.set_xlabel(var)\n",
    "        if plot_type == \"boxplot\":\n",
    "            sns.boxplot(df[var], color='steelblue')\n",
    "        elif plot_type == \"histogram\":\n",
    "            sns.distplot(df[var], kde=True, hist=True)\n",
    "        else:\n",
    "            print(\"Plot type is incorrect.\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def skew_values(df):\n",
    "    \"\"\"\n",
    "    Calculates skewness of all dataframe columns\n",
    "    \"\"\"\n",
    "    return df.skew(axis=0, skipna=True)\n",
    "\n",
    "def color_row(row):\n",
    "    \"\"\"\n",
    "    Color code skewness table, so it's easier to identify abnormal distributions\n",
    "    \"\"\"\n",
    "    return ['background-color: #8a2be2' if (v < 0.5) & (v > -0.5)\n",
    "            else 'background-color: #a2a2d0' if (v > -1) & (v < -0.5) | (v < 1) & (v > 0.5) else \"\" for i, v in\n",
    "            row.items()]\n",
    "\n",
    "\n",
    "def transformations(df):\n",
    "    \"\"\"\n",
    "    Outputs transformed dataframes.\n",
    "    :param: df\n",
    "    :return: df, Log df, Box-Cox df, Sqrt df, QuantileTransformer (Uniform) df, QuantileTransformer (Normal) df, Normalizer() df\n",
    "    \"\"\"\n",
    "    numeric_col = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    df2 = df[numeric_col]\n",
    "\n",
    "    if (df2 <= 0).values.any():\n",
    "        log_df = df2.apply(lambda x: np.log(x + 0.001))\n",
    "        box_cox_df = df2.apply(lambda x: boxcox1p(x, 0.25))\n",
    "    else:\n",
    "        print(\"everything is positive\")\n",
    "        log_df = df2.apply(lambda x: np.log(x))\n",
    "        box_cox_df = df2.apply(lambda x: stats.boxcox(x)[0])\n",
    "\n",
    "    sqrt_df = df2.apply(lambda x: np.sqrt(x))\n",
    "    quant_uniform_df = pd.DataFrame(QuantileTransformer(output_distribution=\"uniform\").fit_transform(df2),\n",
    "                                    columns=df2.columns)\n",
    "    quant_normal_df = pd.DataFrame(QuantileTransformer(output_distribution=\"normal\").fit_transform(df2),\n",
    "                                   columns=df2.columns)\n",
    "    normalizer_df = pd.DataFrame(Normalizer().fit_transform(df2), columns=df2.columns)\n",
    "\n",
    "    return df2, log_df, box_cox_df, sqrt_df, quant_uniform_df, quant_normal_df, normalizer_df\n",
    "\n",
    "\n",
    "def compare_skewness(df):\n",
    "    \"\"\"\n",
    "    :param: df\n",
    "    :return: table containing the skewness of each transformation distribution for all df columns\n",
    "    cells highlighted in purple have approximately symmetric distributions\n",
    "    cells highlighted in lilac have moderately skewed distributions\n",
    "    \"\"\"\n",
    "    df, log_df, box_cox_df, sqrt_df, quant_uniform_df, quant_normal_df, normalizer_df = transformations(df)\n",
    "    sk_table = pd.concat(\n",
    "        [skew_values(df), skew_values(log_df), skew_values(box_cox_df), skew_values(sqrt_df),\n",
    "         skew_values(quant_uniform_df), skew_values(quant_normal_df),\n",
    "         skew_values(normalizer_df)], axis=1)\n",
    "    sk_table = sk_table.rename(\n",
    "        columns={0: 'Regular', 1: \"Log\", 2: \"Box-cox\", 3: \"Sqrt\", 4: \"QuantileTransformer (Uniform)\",\n",
    "                 5: \"QuantileTransformer (Normal)\", 6: \"Normalizer\"})\n",
    "\n",
    "    print(\"If skewness is less than -1 or greater than 1, the distribution is highly skewed.\"\n",
    "          \"\\nIf skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\"\n",
    "          \"\\nIf skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\")\n",
    "    sk_table = sk_table.style.apply(color_row,\n",
    "                                    subset=['Regular', \"Log\", \"Box-cox\", \"Sqrt\", \"QuantileTransformer (Uniform)\",\n",
    "                                            \"QuantileTransformer (Normal)\", \"Normalizer\"], axis=1)\n",
    "    return sk_table\n",
    "\n",
    "\n",
    "def plot_transformations(df, transformation_list, numerical_cols=None):\n",
    "    \"\"\"\n",
    "    :param: df\n",
    "    :param: transformation_list options: ['regular', 'log', 'boxcox', 'sqrt', 'QuantileTransformer (Uniform)', 'QuantileTransformer (Normal)', 'Normalizer']\n",
    "    :param: numerical_cols: list of features to plot. if None, all numerical features in data frame will be plotted.\n",
    "    :return: Histograms representing specified transformations of all numerical features in df (unless numerical_cols is specified)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(24, 15))\n",
    "\n",
    "    if numerical_cols is None:\n",
    "        df, log_df, box_cox_df, sqrt_df, quant_uniform_df, quant_normal_df, normalizer_df = transformations(df)\n",
    "        num_plots = len(df.axes[1])\n",
    "    else:\n",
    "        df2 = df[numerical_cols]\n",
    "        df, log_df, box_cox_df, sqrt_df, quant_uniform_df, quant_normal_df, normalizer_df = transformations(df2)\n",
    "        num_plots = len(df.axes[1])\n",
    "\n",
    "    d = {'regular': df, 'log': log_df, 'boxcox': box_cox_df, 'sqrt': sqrt_df,\n",
    "         \"QuantileTransformer (Uniform)\": quant_uniform_df, \"QuantileTransformer (Normal)\": quant_normal_df,\n",
    "         \"Normalizer\": normalizer_df}\n",
    "\n",
    "    df_list = [d[x] for x in transformation_list]\n",
    "\n",
    "    num_cols = math.ceil(np.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "    for i in range(num_plots):\n",
    "        var = df.columns.tolist()[i]\n",
    "        sub = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "        sub.set_xlabel(var)\n",
    "        for df in df_list:\n",
    "            sns.distplot(df[var], kde=True, hist=True)\n",
    "    fig.legend(labels=transformation_list)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_scalars(df, numerical_cols=None):\n",
    "    \"\"\"\n",
    "    :param: df\n",
    "    :param: numerical_cols: list of features to plot. if None, all numerical features in data frame will be plotted.\n",
    "    :return: Histograms representing 4 different scalars of all numerical features in df (unless numerical_cols is specified)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(24, 15))\n",
    "\n",
    "    if numerical_cols is None:\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        df2 = df[numeric_cols]\n",
    "    else:\n",
    "        df2 = df[numerical_cols]\n",
    "\n",
    "    standard_df = pd.DataFrame(StandardScaler().fit_transform(df2.values), columns=df2.columns, index=df2.index)\n",
    "    min_max_df = pd.DataFrame(MinMaxScaler().fit_transform(df2.values), columns=df2.columns, index=df2.index)\n",
    "    robust_df = pd.DataFrame(RobustScaler().fit_transform(df2.values), columns=df2.columns, index=df2.index)\n",
    "\n",
    "    num_plots = len(df2.axes[1])\n",
    "    num_cols = math.ceil(np.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "    for i in range(num_plots):\n",
    "        var = df2.columns.tolist()[i]\n",
    "        sub = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "        sub.set_xlabel(var)\n",
    "        sns.distplot(df2[var], kde=True, hist=True)\n",
    "        sns.distplot(standard_df[var], kde=True, hist=True)\n",
    "        sns.distplot(min_max_df[var], kde=True, hist=True)\n",
    "        sns.distplot(robust_df[var], kde=True, hist=True)\n",
    "\n",
    "    fig.legend(labels=['Regular', 'Standard', 'Min Max', 'Robust'])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def one_hot_encoder(data, feature):\n",
    "    \"\"\"\n",
    "    :param: df, numerical column\n",
    "    :return: encoded column\n",
    "    \"\"\"\n",
    "    oh = OneHotEncoder()\n",
    "    oh_df = pd.DataFrame(oh.fit_transform(data[[feature]]).toarray())\n",
    "    oh_df.columns = oh.get_feature_names()\n",
    "    for col in oh_df.columns:\n",
    "        oh_df.rename({col: f'{feature}_' + col.split('_')[1]}, axis=1, inplace=True)\n",
    "    return oh_df\n",
    "\n",
    "\n",
    "def outliers_iqr(df, numerical_cols=None):\n",
    "    \"\"\"\n",
    "    Uses IQR to identify outlier data points for each column.\n",
    "    Prints entire index list of outliers and the common index list of outliers for all features.\n",
    "    :param df: Data frame to analyze for outliers.\n",
    "    :param numerical_cols: If argument is None, all numerical columns will be analyzed for outliers.\n",
    "    :return: outliers_df which contains just the outliers, cleaned_df which is resulting data frame with no outliers.\n",
    "    \"\"\"\n",
    "    outliers_list = []\n",
    "    if numerical_cols is None:\n",
    "        col_list = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    else:\n",
    "        col_list = numerical_cols\n",
    "    for feature in col_list:\n",
    "        Q1 = np.percentile(df[feature], 25)\n",
    "        Q3 = np.percentile(df[feature], 75)\n",
    "        step = (Q3 - Q1) * 1.5\n",
    "        print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "        outliers = list(df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))].index.values)\n",
    "        print((df[feature].loc[df[feature].index[outliers]]).tolist())\n",
    "        outliers_list.extend(outliers)\n",
    "\n",
    "    print(\"\\n Index List of Outliers -> {}\".format(outliers_list))\n",
    "    duplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\n",
    "    duplicate_outliers_list.sort()\n",
    "    print(\"\\n Index List of Common Outliers -> {}\".format(duplicate_outliers_list))\n",
    "\n",
    "    # Select the indices for data points you wish to remove\n",
    "    outliers_df = df.iloc[duplicate_outliers_list, :]\n",
    "    cleaned_df = df.drop(df.index[duplicate_outliers_list]).reset_index(drop=True)\n",
    "    return outliers_df, cleaned_df\n",
    "\n",
    "def calc_drop(res):\n",
    "    \"\"\"\n",
    "    Calculates instances of multicollineatiry between pairs of columns in data frame.\n",
    "    \"\"\"\n",
    "    # All variables with correlation > cutoff\n",
    "    all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "\n",
    "    # All unique variables in drop column\n",
    "    poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "    # Keep any variable not in drop column\n",
    "    keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "\n",
    "    # Drop any variables in same row as a keep variable\n",
    "    p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "    q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "    drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "    # Remove drop variables from possible drop\n",
    "    poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "\n",
    "    # subset res dataframe to include possible drop pairs\n",
    "    m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "\n",
    "    # remove rows that are decided (drop), take set and add to drops\n",
    "    more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "    for item in more_drop:\n",
    "        drop.append(item)\n",
    "\n",
    "    return drop\n",
    "\n",
    "\n",
    "def corr_x_new(df, cut = 0.9) :\n",
    "    \"\"\"\n",
    "    Decides which columns to drop from data frame due to multicollineaity\n",
    "    \"\"\"\n",
    "    # Get correlation matrix and upper triangle\n",
    "    corr_mtx = df.corr().abs()\n",
    "    avg_corr = corr_mtx.mean(axis = 1)\n",
    "    up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool))\n",
    "    dropcols = list()\n",
    "    res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target',\n",
    "                                 'v2.target','corr', 'drop' ]))\n",
    "    for row in range(len(up)-1):\n",
    "        col_idx = row + 1\n",
    "        for col in range (col_idx, len(up)):\n",
    "            if corr_mtx.iloc[row, col] > cut:\n",
    "                if avg_corr.iloc[row] > avg_corr.iloc[col]:\n",
    "                    dropcols.append(row)\n",
    "                    drop = corr_mtx.columns[row]\n",
    "                else:\n",
    "                    dropcols.append(col)\n",
    "                    drop = corr_mtx.columns[col]\n",
    "                s = pd.Series([ corr_mtx.index[row], up.columns[col], avg_corr[row], avg_corr[col],\n",
    "                                up.iloc[row,col], drop], index = res.columns)\n",
    "                res = res.append(s, ignore_index = True)\n",
    "    print(res)\n",
    "    dropcols_names = calc_drop(res)\n",
    "    return dropcols_names\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Functions for creating merged dataset\n",
    "\n",
    "def rotate(vector, theta, rotation_around=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    reference: https://en.wikipedia.org/wiki/Rotation_matrix#In_two_dimensions\n",
    "    :param vector: list of length 2 OR\n",
    "                   list of list where inner list has size 2 OR\n",
    "                   1D numpy array of length 2 OR\n",
    "                   2D numpy array of size (number of points, 2)\n",
    "    :param theta: rotation angle in degree (+ve value of anti-clockwise rotation)\n",
    "    :param rotation_around: \"vector\" will be rotated around this point,\n",
    "                    otherwise [0, 0] will be considered as rotation axis\n",
    "    :return: rotated \"vector\" about \"theta\" degree around rotation\n",
    "             axis \"rotation_around\" numpy array\n",
    "    \"\"\"\n",
    "    vector = np.array(vector)\n",
    "    if vector.ndim == 1:\n",
    "        vector = vector[np.newaxis, :]\n",
    "\n",
    "    if rotation_around is not None:\n",
    "        vector = vector - rotation_around\n",
    "\n",
    "    vector = vector.T\n",
    "    theta = np.radians(theta)\n",
    "\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(theta), np.sin(theta)],\n",
    "        [-np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "    output: np.ndarray = (rotation_matrix @ vector).T\n",
    "\n",
    "    if rotation_around is not None:\n",
    "        output = output + rotation_around\n",
    "    return output.squeeze()\n",
    "\n",
    "\n",
    "def get_tracking_data():\n",
    "    \"\"\"\n",
    "    Ask user to select tracking .dat file\n",
    "    Delete \"bad\" data points from this file that are out of spec\n",
    "    Rotate (x,y) 45 degrees clockwise\n",
    "    Round rotated data points to two decimal points\n",
    "    \"\"\"\n",
    "    root = tkinter.Tk()\n",
    "    root.withdraw()\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Select a tracking file...\")\n",
    "            tracking_dat = tkinter.filedialog.askopenfilename(title=\"Select a tracking .dat file\")\n",
    "            tracking_dat_filename = Path(tracking_dat).stem\n",
    "            tracking_df = pd.read_csv(tracking_dat, delimiter=',', skiprows=28, header=None)\n",
    "            # Raise exception if wrong type of file was selected\n",
    "            if len(tracking_df.columns) != 6:\n",
    "                raise Exception\n",
    "            tracking_df_filter = tracking_df[tracking_df[\"I_S\"] > 100]\n",
    "            tracking_df_filter = tracking_df_filter.reset_index(drop=True)\n",
    "            tracking_x_y = tracking_df_filter[[\"X(mm)\", \"Y(mm)\"]].to_numpy()\n",
    "            tracking_x_y_rotate = rotate(tracking_x_y, -45)\n",
    "            tracking_df_filter[\"Tracking X(mm) Rotate\"], tracking_df_filter[\"Tracking Y(mm) Rotate\"] = tracking_x_y_rotate[:, 0], tracking_x_y_rotate[:, 1]\n",
    "            tracking_df_filter[\"Tracking X(mm) Rotate\"] = tracking_df_filter[\"Tracking X(mm) Rotate\"].apply(lambda x: round(x, 2))\n",
    "            tracking_df_filter[\"Tracking Y(mm) Rotate\"] = tracking_df_filter[\"Tracking Y(mm) Rotate\"].apply(lambda x: round(x, 2))\n",
    "            return tracking_dat_filename, tracking_df_filter\n",
    "        except:\n",
    "            print(\"You must select a valid tracking file.\")\n",
    "\n",
    "\n",
    "def get_results_data():\n",
    "    \"\"\"\n",
    "    Ask user to select results .dat file\n",
    "    Delete \"bad\" data points from this file that are out of spec\n",
    "    Rotate (x,y) 45 degrees clockwise\n",
    "    Round rotated data points to two decimal points\n",
    "    \"\"\"\n",
    "    root = tkinter.Tk()\n",
    "    root.withdraw()\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Select a results .dat file...\")\n",
    "            results_dat = tkinter.filedialog.askopenfilename(title=\"Select a results .dat file\")\n",
    "            results_dat_filename = Path(results_dat).stem\n",
    "            results_df = pd.read_csv(results_dat, delimiter=',', skiprows=30, header=None)\n",
    "            # Raise exception if wrong type of file was selected\n",
    "            if len(results_df.columns) != 8:\n",
    "                raise Exception\n",
    "            results_df_filter = results_df[(results_df['Dip'] < 1495) & (results_df['Dip'] > 1220)]\n",
    "            results_df_filter = results_df_filter.reset_index(drop=True)\n",
    "            results_x_y = results_df_filter[[\"X(mm)\", \"Y(mm)\"]].to_numpy()\n",
    "            results_x_y_rotate = rotate(results_x_y, -45)\n",
    "            results_df_filter[\"Results X(mm) Rotate\"], results_df_filter[\"Results Y(mm) Rotate\"] = results_x_y_rotate[:,\n",
    "                                                                                     0], results_x_y_rotate[:, 1]\n",
    "            results_df_filter[\"Results X(mm) Rotate\"] = results_df_filter[\"Results X(mm) Rotate\"].apply(lambda x: round(x, 2))\n",
    "            results_df_filter[\"Results Y(mm) Rotate\"] = results_df_filter[\"Results Y(mm) Rotate\"].apply(lambda x: round(x, 2))\n",
    "            return results_dat_filename, results_df_filter\n",
    "        except:\n",
    "            print(\"You must select a valid results dat file.\")\n",
    "\n",
    "\n",
    "def get_map():\n",
    "    \"\"\"\n",
    "    Upload map into dataframe\n",
    "    \"\"\"\n",
    "    map_df = pd.read_excel(r\"..\\map.xlsx\")\n",
    "    map_df['Map x (mm)'] = map_df['Map x (mm)'].apply(lambda x: round(x, 2))\n",
    "    map_df['Map y (mm)'] = map_df['Map y (mm)'].apply(lambda x: round(x, 2))\n",
    "    return map_df\n",
    "\n",
    "\n",
    "def nearest_neighbors(coordinates, map_df):\n",
    "    \"\"\"\n",
    "    Determine the nearest set of rotated Tracking/Results coordinates for each set of (x,y) coordinates on the  map\n",
    "    \"\"\"\n",
    "    map_df = map_df[[\"Map x (mm)\", \"Map y (mm)\"]].to_numpy()\n",
    "    # Subset last two columns of coordinates which are the rotated X & Y coordinates\n",
    "    coordinates_sub = coordinates.iloc[:, -2:]\n",
    "    # Convert rotated X & Y coordinates columns to numpy array\n",
    "    coordinates_sub = coordinates_sub.to_numpy()\n",
    "    list_of_dicts = []\n",
    "    for row in map_df:\n",
    "        # construct a kd-tree of rotated coordinates\n",
    "        map_tree = scipy.spatial.cKDTree(coordinates_sub)\n",
    "        # find nearest neighbors for each map coordinate within kd-tree of tracking/results rotated coordinates\n",
    "        distance, index = map_tree.query(row)\n",
    "        # Create a row entry in data frame\n",
    "        cols = [\"Map X\", \"Map Y\", \"X Rotate\", \"Y Rotate\", \"Distance\"]\n",
    "        map_x = row[0]\n",
    "        map_y = row[1]\n",
    "        coords_x = (coordinates_sub[index]).flat[0]\n",
    "        coords_y = (coordinates_sub[index]).flat[1]\n",
    "        results = [map_x, map_y, coords_x, coords_y, distance]\n",
    "        results_dict = dict(zip(cols, results))\n",
    "        list_of_dicts.append(results_dict)\n",
    "    results_df = pd.DataFrame(list_of_dicts)\n",
    "    # print(results_df)\n",
    "    return results_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating mapped dataset\n",
    "\n",
    "tracking_dat_filename, tracking_df = get_tracking_data()\n",
    "results_dat_filename, results_df = get_results_data()\n",
    "map_df = get_map()\n",
    "\n",
    "tracking_map_df = nearest_neighbors(tracking_df, map_df)\n",
    "results_map_df = nearest_neighbors(results_df, map_df)\n",
    "\n",
    "# Create resulting dataframes\n",
    "\n",
    "# Tracking\n",
    "tracking_merge_df = pd.merge(map_df, tracking_map_df, on=['Map x (mm)', 'Map y (mm)'], how=\"left\")\n",
    "final_tracking_merge_df = pd.merge(tracking_merge_df, tracking_df, on=['Tracking X(mm) Rotate', 'Tracking Y(mm) Rotate'], how=\"left\")\n",
    "\n",
    "# Results\n",
    "results_merge_df = pd.merge(map_df, results_map_df, on=['Map x (mm)', 'Map y (mm)'], how=\"left\")\n",
    "final_results_merge_df = pd.merge(results_merge_df, results_df, on=['Results X(mm) Rotate', 'Results Y(mm) Rotate'],how=\"left\")\n",
    "\n",
    "\n",
    "# Combine Tracking & Results to create mapped_df\n",
    "mapped_df = pd.merge(final_tracking_merge_df, final_results_merge_df,\n",
    "                      on=['Gx', 'Gy', 'Device', 'Map x (mm)', ' Map y (mm)'], how=\"left\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_df = pd.read_excel(r\"..\\test_data.xlsx\")\n",
    "print(\"Test df shape:\", test_df.shape)\n",
    "\n",
    "# Merge test data with mapped data to create df for project\n",
    "merged_df = test_df.merge(mapped_df, left_on=['X', 'Y', 'ID'], right_on=['Gx', 'Gy', 'ID'], how='left')\n",
    "\n",
    "print(\"Merged df shape:\", merged_df.shape)\n",
    "print(merged_df['ID'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find duplicate columns in merged df\n",
    "duplicateColumnNames = get_duplicate_columns(merged_df)\n",
    "print('Duplicate Columns:')\n",
    "for col in duplicateColumnNames:\n",
    "    print(col)\n",
    "corr_df = merged_df.drop(columns=duplicateColumnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Print columns that only have one constant value\n",
    "# If a column only has one unique value, then it is not important to keep in data frame for modeling purposes\n",
    "bad_cols = (corr_df.columns[corr_df.nunique() <= 2])\n",
    "\n",
    "# Print columns to list to locate other irrelevant columns\n",
    "print(corr_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns that carry the same information & non-relevant columns\n",
    "corr_df_drop = corr_df.copy()\n",
    "corr_df_drop.drop(bad_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Replace negative values (data errors) and zero values with 0.00001 to make transformations easier\n",
    "\n",
    "positive_df = corr_df_drop.copy()\n",
    "num = positive_df._get_numeric_data()\n",
    "num[num <= 0] = 0.00001\n",
    "print(positive_df.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "corr_df_filter = positive_df.copy()\n",
    "\n",
    "# Identify data types\n",
    "print(corr_df_filter.info())\n",
    "\n",
    "# Check number of unique values in the object columns\n",
    "print(corr_df_filter.nunique())\n",
    "\n",
    "# Convert to categorical data types because these are fixed input variables\n",
    "corr_df_filter['Temp'] = corr_df_filter['Temp'].astype('category')\n",
    "corr_df_filter['Lamd'] = corr_df_filter['Lamd'].astype('category')\n",
    "corr_df_filter['Dip'] = corr_df_filter['Dip'].astype('category')\n",
    "corr_df_filter['ID'] = corr_df_filter['ID'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Overall Emitt/OP Plots\n",
    "\n",
    "corr_df_filter_sort = corr_df_filter.sort_values(by=['Emitt'], ascending=True)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 20))\n",
    "emitter_values = corr_df_filter_sort['Emitt'].value_counts().sort_index().values\n",
    "sns.countplot(x='Emitt', order=['6', '12', '25', '28', '34', '41', '48', '62'],\n",
    "              data=corr_df_filter_sort, ax=ax1)\n",
    "for i, p in enumerate(ax1.patches):\n",
    "    height = p.get_height()\n",
    "    ax1.text(p.get_x() + p.get_width() / 2., height + 0.1, emitter_values[i], ha=\"center\")\n",
    "\n",
    "oa_values = corr_df_filter_sort['OP'].value_counts().sort_index().values\n",
    "sns.countplot(x='OP', order=['03k', '05k', '08k', '09k', '11k', '13k', '15k', '18k'],\n",
    "              data=corr_df_filter_sort, ax=ax2)\n",
    "for i, p in enumerate(ax2.patches):\n",
    "    height = p.get_height()\n",
    "    ax2.text(p.get_x() + p.get_width() / 2., height + 0.1, oa_values[i], ha=\"center\")\n",
    "\n",
    "sns.countplot(x='Emitt', color=\"gainsboro\", order=['6', '12', '25', '28', '34', '41', '48', '62'],\n",
    "              data=corr_df_filter_sort, ax=ax3)\n",
    "for i, p in enumerate(ax3.patches):\n",
    "    height = p.get_height()\n",
    "    ax3.text(p.get_x() + p.get_width() / 2., height + 0.1, emitter_values[i], ha=\"center\")\n",
    "sns.countplot(x='Emitt', hue='OP',\n",
    "              hue_order=['03k', '05k', '08k', '09k', '11k', '13k', '15k', '18k'],\n",
    "              data=corr_df_filter_sort, ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Emitt/OP Plots within Recipe\n",
    "\n",
    "fig, (ax4, ax5) = plt.subplots(2, 1, figsize=(20, 15))\n",
    "recipe_values = corr_df_filter_sort['Recipes'].value_counts().sort_index().values\n",
    "sns.countplot(x='Recipes', color=\"gainsboro\",\n",
    "              data=corr_df_filter_sort, ax=ax4)\n",
    "for i, p in enumerate(ax4.patches):\n",
    "    height = p.get_height()\n",
    "    ax4.text(p.get_x() + p.get_width() / 2., height + 0.1, recipe_values[i], ha=\"center\")\n",
    "sns.countplot(x='Recipes', hue='OP',\n",
    "              hue_order=['04um', '03k', '05k', '08k', '09k', '11k', '13k', '15k', '18k'],\n",
    "              data=corr_df_filter_sort, ax=ax4)\n",
    "\n",
    "sns.countplot(x='Recipes', color=\"gainsboro\",\n",
    "              data=corr_df_filter_sort, ax=ax5)\n",
    "for i, p in enumerate(ax5.patches):\n",
    "    height = p.get_height()\n",
    "    ax5.text(p.get_x() + p.get_width() / 2., height + 0.1, recipe_values[i], ha=\"center\")\n",
    "sns.countplot(x='Recipes', hue='Emitt',\n",
    "              hue_order=['6', '12', '19', '28', '34', '41', '48', '62'], data=corr_df_filter_sort, ax=ax5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Grouping Recipe Name by Temperature\n",
    "\n",
    "print(corr_df_filter.groupby(['Recipe Name'])['Temp'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Fixed input variables: Emitt, OP, Current(mA), Dip, Lamd, Temperature, Recipes, Recipe Name, ID\n",
    "# Converting Emitt, OP, Current(mA), Recipe, Recipe Name into categorical variables\n",
    "# Dip, Lamd, Temperature were converted into categorical variables earlier\n",
    "\n",
    "# Combo of (Emitt, OP)\n",
    "# There are 41 unique combinations of Emitt and OP\n",
    "\n",
    "corr_df_filter['Emitt:OP'] = corr_df_filter[['Emitt', 'OP']].apply(\n",
    "    lambda row: ':'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "corr_df_filter['Emitt:OP'] = corr_df_filter['Emitt:OP'].astype('category')\n",
    "\n",
    "corr_df_filter = corr_df_filter.drop(['Emitt','OP'], axis=1)\n",
    "\n",
    "# corr_df_filter['Emitt'] = corr_df_filter['Emitt'].str[:-1]\n",
    "# corr_df_filter['OP'] = corr_df_filter['OP'].str[:-2]\n",
    "\n",
    "corr_df_filter['Recipe Name'] = corr_df_filter['Recipe Name'].astype('category')\n",
    "corr_df_filter['Recipes'] = corr_df_filter['Recipes'].astype('category')\n",
    "corr_df_filter['ID'] = corr_df_filter['ID'].astype('category')\n",
    "corr_df_filter['Emitt'] = corr_df_filter['Emitt'].astype('category')\n",
    "corr_df_filter['OP'] = corr_df_filter['OP'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Current(mA)\n",
    "# There are 37 unique combinations of fixed inputs chosen from Steps1-7\n",
    "# Assign each unique combination to a letter\n",
    "# Convert individual Current_Step columns into category variables to track individual steps\n",
    "\n",
    "gp_current = (corr_df_filter.groupby(\n",
    "    ['Current(mA)_1', 'Current(mA)_2', 'Current(mA)_Step3', 'Current(mA)_4',\n",
    "     'Current(mA)_Step5', 'Current(mA)_6', 'Current(mA)_Step7']).size().reset_index(name='Freq'))\n",
    "gp_current.drop(gp_current.columns[len(gp_current.columns) - 1], axis=1, inplace=True)\n",
    "gp_current_keys = gp_current.values.tolist()\n",
    "\n",
    "\n",
    "count = 0\n",
    "d = {}\n",
    "for i in gp_current_keys:\n",
    "    if count == 26:\n",
    "        count += 6\n",
    "    if tuple(i) not in d:\n",
    "        d[tuple(i)] = chr(ord('A') + count)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "df = corr_df_filter.copy()\n",
    "df['Current(mA)_StepType'] = df.apply(\n",
    "    lambda row: d[(row['Current(mA)_1'], row['Current(mA)_2'], row['Current(mA)_3'],\n",
    "                   row['Current(mA)_4'], row['Current(mA)_5'], row['Current(mA)_6'],\n",
    "                   row['Current(mA)_7'])], axis=1)\n",
    "\n",
    "for col in ['Current(mA)_StepType', 'Current(mA)_1', 'Current(mA)_2',\n",
    "            'Current(mA)_3', 'Current(mA)_4',\n",
    "            'Current(mA)_5', 'Current(mA)_6', 'Current(mA)_7']:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ID plot\n",
    "\n",
    "wafer_sort = df.sort_values(by=['ID'], ascending=True)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(30, 15))\n",
    "wafer_values = wafer_sort['ID'].value_counts().sort_index().values\n",
    "sns.countplot(x='ID', data=wafer_sort, ax=ax1)\n",
    "for i, p in enumerate(ax1.patches):\n",
    "    height = p.get_height()\n",
    "    ax1.text(p.get_x() + p.get_width() / 2., height + 0.1, wafer_values[i], ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Emitt:OP Counts\n",
    "\n",
    "fig, (ax) = plt.subplots(1, 1, figsize=(30, 15))\n",
    "emoa_sort = df.sort_values(by=['Emitt:OP'], ascending=True)\n",
    "emoa_sort_val = emoa_sort['Emitt:OP'].unique()\n",
    "print(emoa_sort_val)\n",
    "\n",
    "sns.countplot(x='Emitt:OP', data=emoa_sort, order=emoa_sort_val, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_title('Emitt:OP Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# X & Y by Temperature\n",
    "\n",
    "g2 = sns.FacetGrid(df, col=\"Temp\", height=10, aspect=1)\n",
    "g2.map(sns.scatterplot, \"X\", \"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot Current(mA)_StepType\n",
    "\n",
    "sns.set(rc={'figure.figsize': (20, 10)})\n",
    "g3 = sns.countplot(x='Current(mA)_StepType', data=df)\n",
    "g3.set_title('Current(mA)_StepType Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Lamd/Dip\n",
    "\n",
    "print(\"Most frequent Lamd values:\", df['Lamd'].value_counts().index.tolist())\n",
    "g4 = sns.countplot(x='Lamd', data=df)\n",
    "g4.set_title('Lamd Input Counts')\n",
    "plt.show()\n",
    "\n",
    "print(\"Most frequent Dip values:\", df['Dip'].value_counts().index.tolist())\n",
    "g5 = sns.countplot(x='Dip', data=df)\n",
    "g5.set_title('Dip Input Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Temp plot\n",
    "\n",
    "g6 = sns.countplot(x='Temp', data=df)\n",
    "g6.set_title('Temp Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Recipes plot\n",
    "\n",
    "g7 = sns.countplot(x='Recipes', data=df)\n",
    "g7.set_title('Recipes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Y variation within Fp-Dip and Lamd_2\n",
    "\n",
    "sns.lineplot(data=df, x=\"Y\", y=\"Dip\")\n",
    "plt.show()\n",
    "sns.lineplot(data=df, x=\"Y\", y=\"Lamd_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Boxplots showing relationships between all categorical variables and WPE\n",
    "\n",
    "categorical_features = (df.select_dtypes(include=['category']).columns.tolist())\n",
    "\n",
    "fig = plt.figure(figsize=(20, 50))\n",
    "for i in range(len(categorical_features)):\n",
    "    var = categorical_features[i]\n",
    "    fig.add_subplot(11, 4, i + 1)\n",
    "    sns.boxplot(y=df['WPE'], x=df[var])\n",
    "plt.title('Correlations Between WPE and all Categorical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix between all numerical variables\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "mask = np.zeros_like(df.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(df.corr(), cmap=sns.diverging_palette(20, 220, n=200),\n",
    "            mask=mask, annot=True, center=0)\n",
    "plt.title('Correlations Between all Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove 'Recipe Name' and 'Recipes' column\n",
    "\n",
    "df = df.drop(columns=['Recipe Name', 'Recipes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encode\n",
    "\n",
    "emitt_op_encode = one_hot_encoder(df, 'Emitt:OP')\n",
    "# fpdip_encode = one_hot_encoder(df, 'Dip')\n",
    "# lambda_encode = one_hot_encoder(df, 'Lamd')\n",
    "current_encode = one_hot_encoder(df, 'Current(mA)_StepType')\n",
    "temp_encode = one_hot_encoder(df, 'Temp')\n",
    "emitt_encode = one_hot_encoder(df, 'Emitt')\n",
    "op_encode = one_hot_encoder(df, 'OP')\n",
    "\n",
    "one_hot_encode = pd.concat([\n",
    "emitt_op_encode, current_encode, temp_encode, emitt_encode, op_encode], axis=1)\n",
    "one_hot_encode.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "encoded_df = pd.merge(df, one_hot_encode, left_index=True, right_index=True)\n",
    "encoded_df = encoded_df.drop([\n",
    "    'Emitt:OP', 'Current(mA)_StepType', 'Temp', 'Emitt', 'OP',], axis=1)\n",
    "encoded_df.columns = encoded_df.columns.str.replace(']', ')')\n",
    "print(encoded_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['X', 'Y', 'Voltage(V)_1', 'Voltage(V)_2', 'Voltage(V)_3', 'Voltage(V)_4',\n",
    " 'Voltage(V)_5', 'Voltage(V)_6', 'Voltage(V)_7', 'Power_1', 'Power_2',\n",
    " 'Power_3', 'Power_4', 'Power_5', 'Power_6', 'Power_7', 'WPE', 'Ith',\n",
    " 'Lamd_2', 'SpecWR', 'SpecW', 'Ith_I', 'Dense_V',\n",
    " 'Dense_VI', 'Peak_I', 'I_S']\n",
    "\n",
    "# Correlation Matrix for numerical variables\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "mask = np.zeros_like(df[num_cols].corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(df[num_cols].corr(), cmap=sns.diverging_palette(20, 220, n=200),\n",
    "            mask=mask, annot=True, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove outliers from numerical columns using IQR method\n",
    "\n",
    "df2 = df.copy()\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df2_outliers, df2_cleaned = outliers_iqr(df2, numerical_cols = num_cols)\n",
    "\n",
    "# Boxplots before outlier removal\n",
    "multiple_features_plots(df2, \"boxplot\", num_cols)\n",
    "\n",
    "# Boxplots after outlier removal\n",
    "multiple_features_plots(df2_cleaned, \"boxplot\", num_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initial correlation results\n",
    "corr_results(df2_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate skewness of each variable for various distributions\n",
    "compare_skewness(df2_cleaned[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transform necessary variables using box-cox transformation\n",
    "transform_cols = ['Power_1',  'Ith', 'SpecWR', 'Ith_I']\n",
    "# Plot transformation\n",
    "plot_transformations(df2_cleaned, numerical_cols=transform_cols, transformation_list=['regular', 'boxcox'])\n",
    "\n",
    "df2_transformed  = df2_cleaned.copy()\n",
    "df2_transformed[transform_cols] = df2_transformed[transform_cols].apply(lambda x: stats.boxcox(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# Shuffle data frame\n",
    "df2_shuffled = df2_transformed.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation with target variable 'WPE'\n",
    "\n",
    "df2_shuffled_corr = corr_target(df2_shuffled, 'WPE')\n",
    "print(df2_shuffled_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Imports for ML\n",
    "\n",
    "import shap\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern, RationalQuadratic\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Designate 'WPE' as the target variable and other variables as predictors\n",
    "\n",
    "wpe_df_x = df2_shuffled.drop('WPE', axis=1)\n",
    "wpe_df_y = df2_shuffled['WPE']\n",
    "\n",
    "# Split into training and testing data sets\n",
    "wpe_x_train, wpe_x_test, wpe_y_train, wpe_y_test = train_test_split(wpe_df_x, wpe_df_y, test_size=0.2, random_state=42)\n",
    "print(f\"Train set has {wpe_x_train.shape[0]} records out of {len(df2_shuffled)} which is {round(wpe_x_train.shape[0] / len(df2_shuffled) * 100)}%\")\n",
    "print(f\"Test set has {wpe_x_test.shape[0]} records out of {len(df2_shuffled)} which is {round(wpe_x_test.shape[0] / len(df2_shuffled) * 100)}%\")\n",
    "\n",
    "print(\"Shape of X_train: \", wpe_x_train.shape)\n",
    "print(\"Shape of X_test: \", wpe_x_test.shape)\n",
    "print(\"Shape of y_train: \", wpe_y_train.shape)\n",
    "print(\"Shape of y_test: \", wpe_y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale numerical features using StandardScaler approach\n",
    "\n",
    "numerical_features = ['X', 'Y', 'Ith', 'Lamd_2', 'SpecWR', 'SpecW', 'Ith_I',\n",
    " 'Dense_V', 'Dense_VI', 'Lamd', 'Peak_I', 'I_S', 'Dip', 'SB Delta']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wpe_x_train[numerical_features] = scaler.fit_transform(wpe_x_train[numerical_features])\n",
    "wpe_x_test[numerical_features] = scaler.transform(wpe_x_test[numerical_features])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop variables from dataset with high multicollinearity\n",
    "\n",
    "corr_drop_cols = corr_x_new(wpe_x_train, cut = 0.9)\n",
    "print(corr_drop_cols)\n",
    "wpe_x_train.drop(columns=corr_drop_cols, axis=1, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression)\n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "\n",
    "# define the grid\n",
    "grid = dict()\n",
    "\n",
    "# Initial test for range of optimal features\n",
    "grid['sel__k'] = [i for i in range(wpe_x_train.shape[1]-60, wpe_x_train.shape[1]+1)]\n",
    "\n",
    "# 113 total features\n",
    "# 76 is the number of optimal features\n",
    "# We will make the range (50,100) for future testing of optimal features\n",
    "# 0.019 MAE begins at 56 features\n",
    "# grid['sel__k'] = [i for i in range(wpe_x_train.shape[1]-63, wpe_x_train.shape[1]-12)]\n",
    "\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "results = search.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print('Best MAE: %.3f' % results.best_score_)\n",
    "print('Best Config: %s' % results.best_params_)\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation of a model using 76 optimal features\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test, k):\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "X_train_fs, X_test_fs, fs = select_features(wpe_x_train, wpe_y_train, wpe_x_test, k=76)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs, wpe_y_train)\n",
    "yhat = model.predict(X_test_fs)\n",
    "mae = mean_absolute_error(wpe_y_test, yhat)\n",
    "print('MAE: %.3f' % mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print 76 chosen features with their F-scores\n",
    "\n",
    "names = wpe_x_train.columns.values[fs.get_support()]\n",
    "scores = fs.scores_[fs.get_support()]\n",
    "names_scores = list(zip(names, scores))\n",
    "ns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])\n",
    "ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])\n",
    "print(ns_df_sorted)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate RMSE for every model type\n",
    "# Pick 3 lowest RMSE and create those models\n",
    "\n",
    "lr_cv = -cross_val_score(LinearRegression(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Linear Regression is: {np.mean(np.sqrt(lr_cv))}')\n",
    "\n",
    "sgd_cv = -cross_val_score(SGDRegressor(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for SGD is: {np.mean(np.sqrt(sgd_cv))}')\n",
    "\n",
    "ridge_cv = -cross_val_score(Ridge(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Ridge Regression is: {np.mean(np.sqrt(ridge_cv))}')\n",
    "\n",
    "lasso_cv = -cross_val_score(Lasso(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Lasso Regression is: {np.mean(np.sqrt(lasso_cv))}')\n",
    "\n",
    "elastic_cv = -cross_val_score(ElasticNet(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for ElasticNet Regression is: {np.mean(np.sqrt(elastic_cv))}')\n",
    "\n",
    "dt_cv = -cross_val_score(DecisionTreeRegressor(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Decision Tree Regression is: {np.mean(np.sqrt(dt_cv))}')\n",
    "\n",
    "rf_cv = -cross_val_score(RandomForestRegressor(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Random Forest Regression is: {np.mean(np.sqrt(rf_cv))}')\n",
    "\n",
    "svr_cv = -cross_val_score(SVR(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for SVR is: {np.mean(np.sqrt(svr_cv))}')\n",
    "\n",
    "ada_cv = -cross_val_score(AdaBoostRegressor(), wpe_x_train, wpe_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for AdaBoost Regressor is: {np.mean(np.sqrt(ada_cv))}')\n",
    "\n",
    "gaussian_cv = -cross_val_score(GaussianProcessRegressor(), wpe_x_train, wpe_y_train, cv=10,\n",
    "                               scoring='neg_mean_squared_error')\n",
    "print(f'\\nThe average RMSE for Gaussian Process Regressor is: {np.mean(np.sqrt(gaussian_cv))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot cross validation results\n",
    "def plotCvResults(model_cv):\n",
    "    cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "    cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "    plt.plot(np.log1p(cv_results['param_alpha']), cv_results['mean_train_score'])\n",
    "    plt.plot(np.log1p(cv_results['param_alpha']), cv_results['mean_test_score'])\n",
    "    plt.xlabel('log1p(alpha)')\n",
    "    plt.ylabel('Negative Mean Absolute Error')\n",
    "    plt.title(\"Negative Mean Absolute Error and log1p(alpha)\")\n",
    "    plt.legend(['train score', 'test score'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# display parameters\n",
    "def bestParams(model, X_train):\n",
    "    print(\"Best Alpha for Regularized Regression:\", model.get_params()['alpha'])\n",
    "    model_parameters = [abs(x) for x in list(model.coef_)]\n",
    "    model_parameters.insert(0, model.intercept_)\n",
    "    model_parameters = [round(x, 3) for x in model_parameters]\n",
    "    cols = X_train.columns\n",
    "    cols = cols.insert(0, \"constant\")\n",
    "    model_coef = sorted(list(zip(cols, model_parameters)), key=operator.itemgetter(1), reverse=True)[:11]\n",
    "    print(\"Top 10 Model parameters (excluding constant) are:\")\n",
    "    for p, c in model_coef:\n",
    "        print(p)\n",
    "\n",
    "def modelR2AndSpread(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(\"Train R^2:\", r2_score(y_true=y_train, y_pred=y_train_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print(\"Test R^2:\", r2_score(y_true=y_test, y_pred=y_test_pred))\n",
    "    print('RMSE Train: ' + str(np.sqrt(mean_squared_error(y_train, y_train_pred))))\n",
    "    print('RMSE Test: ' + str(np.sqrt(mean_squared_error(y_test, y_test_pred))))\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    plt.suptitle(\"Linear Regression Assumptions\", fontsize=16)\n",
    "\n",
    "    # plot error spread\n",
    "    fig.add_subplot(2, 2, 1)\n",
    "    sns.regplot(y_train, y_train_pred)\n",
    "    plt.title('y_train vs y_train_pred', fontsize=14)\n",
    "    plt.xlabel('y_train', fontsize=12)\n",
    "    plt.ylabel('y_train_pred', fontsize=12)\n",
    "\n",
    "    fig.add_subplot(2, 2, 2)\n",
    "    sns.regplot(y_test, y_test_pred)\n",
    "    plt.title('y_test vs y_test_pred', fontsize=14)\n",
    "    plt.xlabel('y_test', fontsize=12)\n",
    "    plt.ylabel('y_test_pred', fontsize=12)\n",
    "\n",
    "    # plot residuals for linear regression assumption\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    fig.add_subplot(2, 2, 3)\n",
    "    sns.distplot(residuals_train)\n",
    "    plt.title('residuals between y_train & y_train_pred', fontsize=14)\n",
    "    plt.xlabel('residuals', fontsize=12)\n",
    "\n",
    "    fig.add_subplot(2, 2, 4)\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    sns.distplot(residuals_test)\n",
    "    plt.title('residuals between y_test & y_test_pred', fontsize=14)\n",
    "    plt.xlabel('residuals', fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression \n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print('MAE: ', metrics.mean_absolute_error(wpe_y_test, lin_reg.predict(wpe_x_test)))\n",
    "print('MSE: ', metrics.mean_squared_error(wpe_y_test, lin_reg.predict(wpe_x_test)))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(wpe_y_test, lin_reg.predict(wpe_x_test))))\n",
    "modelR2AndSpread(lin_reg, wpe_x_train, wpe_x_test, wpe_y_train, wpe_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Ridge Regression optimal parameters\n",
    "\n",
    "param_grid = {'alpha': (np.logspace(-8, 8, 100))}  # It will check from 1e-08 to 1e+08\n",
    "\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge_model = GridSearchCV(ridge, param_grid, cv=10)\n",
    "ridge_model.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print(ridge_model.best_params_)\n",
    "print(ridge_model.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridge_cv = Ridge(alpha=5.214008287999695e-05, normalize=True)\n",
    "ridge_cv.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print('MAE: ', metrics.mean_absolute_error(wpe_y_test, ridge_cv.predict(wpe_x_test)))\n",
    "print('MSE: ', metrics.mean_squared_error(wpe_y_test, ridge_cv.predict(wpe_x_test)))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(wpe_y_test, ridge_cv.predict(wpe_x_test))))\n",
    "modelR2AndSpread(ridge_cv, wpe_x_train, wpe_x_test, wpe_y_train, wpe_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Coefficients in the Ridge Model\n",
    "\n",
    "coef = pd.Series(ridge_cv.coef_, index=wpe_x_train.columns)\n",
    "imp_coef = pd.concat([coef.sort_values()])\n",
    "plt.figure(figsize=(15, 10), dpi=100)\n",
    "imp_coef.plot(kind=\"barh\")\n",
    "plt.title(\"Coefficients in the Ridge Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr_random_params = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale']\n",
    "}\n",
    "svr_cv = GridSearchCV(svr, param_grid=svr_random_params,\n",
    "                      scoring='neg_mean_absolute_error', n_jobs=-1, verbose=4)\n",
    "\n",
    "svr_cv.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print(svr_cv.best_params_)\n",
    "print(svr_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine Decision Tree optimal parameters\n",
    "# Max-depth\n",
    "\n",
    "test_r2_score = []\n",
    "train_r2_score = []\n",
    "for i in range(1, 50):\n",
    "    reg = DecisionTreeRegressor(max_depth=i)\n",
    "    reg.fit(wpe_x_train, wpe_y_train)\n",
    "    test_r2_score.append(r2_score(wpe_y_test, reg.predict(wpe_x_test)))\n",
    "    train_r2_score.append(r2_score(wpe_y_train, reg.predict(wpe_x_train)))\n",
    "max_depth_df = pd.DataFrame(\n",
    "    {'max_depth': np.arange(1, 50), 'test_r2_score': test_r2_score, 'train_r2_score': train_r2_score})\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(max_depth_df['max_depth'], max_depth_df['test_r2_score'], marker='o', label='test_r2_score')\n",
    "plt.plot(max_depth_df['max_depth'], max_depth_df['train_r2_score'], marker='o', label='train_r2_score')\n",
    "plt.legend()\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('r2_score')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Min_samples_split\n",
    "\n",
    "Min_samples_split_test_r2_score = []\n",
    "Min_samples_split_train_r2_score = []\n",
    "for i in np.linspace(0.1, 1, 10, endpoint=True):\n",
    "    reg = DecisionTreeRegressor(min_samples_split=i)\n",
    "    reg.fit(wpe_x_train, wpe_y_train)\n",
    "    Min_samples_split_test_r2_score.append(r2_score(wpe_y_test, reg.predict(wpe_x_test)))\n",
    "    Min_samples_split_train_r2_score.append(r2_score(wpe_y_train, reg.predict(wpe_x_train)))\n",
    "Min_samples_split = pd.DataFrame(\n",
    "    {'Min_samples_split': np.linspace(0.1, 1, 10), 'Min_samples_split_test_r2_score': Min_samples_split_test_r2_score,\n",
    "     'Min_samples_split_train_r2_score': Min_samples_split_train_r2_score})\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(Min_samples_split['Min_samples_split'], Min_samples_split['Min_samples_split_test_r2_score'], marker='o',\n",
    "         label='Min_samples_split_test_r2_score')\n",
    "plt.plot(Min_samples_split['Min_samples_split'], Min_samples_split['Min_samples_split_train_r2_score'], marker='o',\n",
    "         label='Min_samples_split_train_r2_score')\n",
    "plt.legend()\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('r2_score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Min_samples_leaf\n",
    "\n",
    "min_samples_leaf_test_r2_score = []\n",
    "min_samples_leaf_train_r2_score = []\n",
    "for i in np.linspace(0.1, 0.5, 5, endpoint=True):\n",
    "    reg = DecisionTreeRegressor(min_samples_leaf=i)\n",
    "    reg.fit(wpe_x_train, wpe_y_train)\n",
    "    min_samples_leaf_test_r2_score.append(r2_score(wpe_y_test, reg.predict(wpe_x_test)))\n",
    "    min_samples_leaf_train_r2_score.append(r2_score(wpe_y_train, reg.predict(wpe_x_train)))\n",
    "min_samples_leaf = pd.DataFrame(\n",
    "    {'min_samples_leaf': np.linspace(0.1, 0.5, 5), 'min_samples_leaf_test_r2_score': min_samples_leaf_test_r2_score,\n",
    "     'min_samples_leaf_train_r2_score': min_samples_leaf_train_r2_score})\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(min_samples_leaf['min_samples_leaf'], min_samples_leaf['min_samples_leaf_test_r2_score'], marker='o',\n",
    "         label='min_samples_leaf_test_r2_score')\n",
    "plt.plot(min_samples_leaf['min_samples_leaf'], min_samples_leaf['min_samples_leaf_train_r2_score'], marker='o',\n",
    "         label='min_samples_leaf_train_r2_score')\n",
    "plt.legend()\n",
    "plt.xlabel('min_samples_leaf')\n",
    "plt.ylabel('r2_score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Max_features\n",
    "\n",
    "max_features_test_r2_score = []\n",
    "max_features_train_r2_score = []\n",
    "for i in list(range(1, wpe_df_x.shape[1])):\n",
    "    reg = DecisionTreeRegressor(max_features=i)\n",
    "    reg.fit(wpe_x_train, wpe_y_train)\n",
    "    max_features_test_r2_score.append(r2_score(wpe_y_test, reg.predict(wpe_x_test)))\n",
    "    max_features_train_r2_score.append(r2_score(wpe_y_train, reg.predict(wpe_x_train)))\n",
    "max_features = pd.DataFrame(\n",
    "    {'max_features': list(range(1, wpe_df_x.shape[1])), 'max_features_test_r2_score': max_features_test_r2_score,\n",
    "     'max_features_train_r2_score': max_features_train_r2_score})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(1, 75), max_features['max_features_test_r2_score'], marker='o', label='max_features_test_r2_score')\n",
    "plt.plot(np.arange(1, 75), max_features['max_features_train_r2_score'], marker='o', label='max_features_train_r2_score')\n",
    "plt.legend()\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('r2_score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Max_leaf_nodes\n",
    "\n",
    "max_leaf_nodes_test_r2_score = []\n",
    "max_leaf_nodes_train_r2_score = []\n",
    "for i in np.arange(1, 110, 10):\n",
    "    reg = DecisionTreeRegressor(min_samples_leaf=i)\n",
    "    reg.fit(wpe_x_train, wpe_y_train)\n",
    "    max_leaf_nodes_test_r2_score.append(r2_score(wpe_y_test, reg.predict(wpe_x_test)))\n",
    "    max_leaf_nodes_train_r2_score.append(r2_score(wpe_y_train, reg.predict(wpe_x_train)))\n",
    "max_leaf_nodes = pd.DataFrame(\n",
    "    {'min_samples_leaf': np.arange(1, 110, 10), 'max_leaf_nodes_test_r2_score': max_leaf_nodes_test_r2_score,\n",
    "     'max_leaf_nodes_train_r2_score': max_leaf_nodes_train_r2_score})\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(1, 110, 10), max_leaf_nodes['max_leaf_nodes_test_r2_score'], marker='o',\n",
    "         label='max_leaf_nodes_test_r2_score')\n",
    "plt.plot(np.arange(1, 110, 10), max_leaf_nodes['max_leaf_nodes_train_r2_score'], marker='o',\n",
    "         label='max_leaf_nodes_train_r2_score')\n",
    "plt.legend()\n",
    "plt.xlabel('max_leaf_nodes')\n",
    "plt.ylabel('r2_score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Decision Tree Regressor with basic Grid Search parameters\n",
    "# Ignores previous cells that calculated Max-depth, Min_samples_split, Min_samples_leaf, Max_features, Max_leaf_nodes\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "grid = {'max_depth': np.arange(10, 30),\n",
    "        \"min_samples_leaf\": np.arange(1, 100, 10),\n",
    "        \"min_samples_split\": np.linspace(0.001, 1, 10)}\n",
    "\n",
    "dt_grid = GridSearchCV(estimator=dt, param_grid=grid, verbose=4)\n",
    "dt_grid.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print('Train R^2 Score : %.3f' % dt_grid.best_estimator_.score(wpe_x_train, wpe_y_train))\n",
    "print('Test R^2 Score : %.3f' % dt_grid.best_estimator_.score(wpe_x_test, wpe_y_test))\n",
    "print('Best R^2 Score Through Grid Search : %.3f' % dt_grid.best_score_)\n",
    "print('Best Parameters : ', dt_grid.best_params_)\n",
    "\n",
    "print('MAE: ', metrics.mean_absolute_error(wpe_y_test, dt_grid.predict(wpe_x_test)))\n",
    "print('MSE: ', metrics.mean_squared_error(wpe_y_test, dt_grid.predict(wpe_x_test)))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(wpe_y_test, dt_grid.predict(wpe_x_test))))\n",
    "modelR2AndSpread(dt_grid, wpe_x_train, wpe_x_test, wpe_y_train, wpe_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot feature importances from DT regressor\n",
    "\n",
    "FI_df = pd.DataFrame({'Features': wpe_x_train.columns, 'Importance': dt_grid.best_estimator_.feature_importances_})\n",
    "FI_df = FI_df.sort_values(by='Importance', ascending=False).head(100)\n",
    "\n",
    "plt.figure(figsize=(15, 10), dpi=100)\n",
    "sns.barplot(x=FI_df[:30].Importance, y=FI_df[:30].Features, orient='h').set_title('Feature Importance')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(wpe_x_train, wpe_y_train)\n",
    "\n",
    "print('MAE: ', metrics.mean_absolute_error(wpe_y_test, rf.predict(wpe_x_test)))\n",
    "print('MSE: ', metrics.mean_squared_error(wpe_y_test, rf.predict(wpe_x_test)))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(wpe_y_test, rf.predict(wpe_x_test))))\n",
    "modelR2AndSpread(rf, wpe_x_train, wpe_x_test, wpe_y_train, wpe_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot feature importances from RF regressor\n",
    "\n",
    "FI_df = pd.DataFrame({'Features': wpe_x_train.columns, 'Importance': rf.feature_importances_})\n",
    "FI_df = FI_df.sort_values(by='Importance', ascending=False).head(100)\n",
    "\n",
    "plt.figure(figsize=(15, 10), dpi=100)\n",
    "sns.barplot(x=FI_df[:30].Importance, y=FI_df[:30].Features, orient='h').set_title('Feature Importance')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap Values\n",
    "# Final step\n",
    "\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "print(explainer)\n",
    "\n",
    "shap_values = explainer.shap_values(wpe_x_test)\n",
    "print(shap_values)\n",
    "\n",
    "shap.summary_plot(shap_values, wpe_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (Still working on!)\n",
    "# Purpose:\n",
    "# Create faster approach for hyperparamter turning using RandomizedSearchCV\n",
    "# All models are then ran with optimal hyperparameters\n",
    "# Data frame is constructed with evaluation metrics for each model\n",
    "\n",
    "n_iter_num = 20\n",
    "cv_num = 3\n",
    "\n",
    "def train_LinearRegression(X_train, y_train):\n",
    "    print('Training Linear Regression ...')\n",
    "    lr = LinearRegression(copy_X=True, positive=False, n_jobs=-1)\n",
    "    lr_random_params = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False]\n",
    "    }\n",
    "    lr_cv = RandomizedSearchCV(lr, param_distributions=lr_random_params, n_iter=n_iter_num, cv=cv_num,\n",
    "                               scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=4)\n",
    "    lr_cv.fit(X_train, y_train)\n",
    "    return lr_cv\n",
    "\n",
    "\n",
    "# higher value for alpha implies that there were some useless features in the model and therefore by increasing alpha the affect of these features has been decreased, decreasing the variance of the model (the model will not overfit as severly)\n",
    "\n",
    "def train_Lasso(X_train, y_train):\n",
    "    print('Training Lasso Regression ...')\n",
    "    lasso = Lasso()\n",
    "    lasso_random_params = {\n",
    "        'alpha': [0.0001, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 1.0, 5.0, 10]\n",
    "    }\n",
    "    lasso_cv = RandomizedSearchCV(lasso, param_distributions=lasso_random_params, n_iter=n_iter_num, cv=cv_num,\n",
    "                                  scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=4)\n",
    "    lasso_cv.fit(X_train, y_train)\n",
    "    return lasso_cv\n",
    "\n",
    "\n",
    "def train_Ridge(X_train, y_train):\n",
    "    print('Training Ridge Regression ...')\n",
    "    ridge = Ridge()\n",
    "    ridge_random_params = {\n",
    "        'alpha': [0.0001, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 1.0, 5.0, 10]\n",
    "    }\n",
    "    ridge_cv = RandomizedSearchCV(ridge, param_distributions=ridge_random_params, n_iter=n_iter_num, cv=cv_num,\n",
    "                                  scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=4)\n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "    return ridge_cv\n",
    "\n",
    "\n",
    "def train_ElasticNet(X_train, y_train):\n",
    "    print('Training Elastic Net ...')\n",
    "    elastic_net = ElasticNet()\n",
    "    elastic_net_random_params = {\n",
    "        'model__alpha': list(np.arange(0, 20, 0.1)),\n",
    "        'model__l1_ratio': list(np.arange(0, 1, 0.001))\n",
    "    }\n",
    "    elastic_net_cv = RandomizedSearchCV(elastic_net, param_distributions=elastic_net_random_params, n_iter=n_iter_num,\n",
    "                                        cv=cv_num,\n",
    "                                        scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=4)\n",
    "    elastic_net_cv.fit(X_train, y_train)\n",
    "    return elastic_net_cv\n",
    "\n",
    "\n",
    "def train_SVR(X_train, y_train):\n",
    "    print('Training SVR ...')\n",
    "    svr = SVR()\n",
    "    svr_random_params = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': scipy.stats.reciprocal(0.001, 100.),\n",
    "        'epsilon': scipy.stats.uniform(0.1, 1.),\n",
    "        'gamma': scipy.stats.reciprocal(0.001, 1.),\n",
    "    }\n",
    "    svr_cv = RandomizedSearchCV(svr, param_distributions=svr_random_params, n_iter=n_iter_num, cv=cv_num,\n",
    "                                scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42, verbose=4)\n",
    "    svr_cv.fit(X_train, y_train)\n",
    "    return svr_cv\n",
    "\n",
    "\n",
    "def train_DecisionTree(X_train, y_train):\n",
    "    print('Training DecisionTree ...')\n",
    "    dt = DecisionTreeRegressor(random_state=0)\n",
    "    dt_random_params = {\n",
    "        'max_depth': scipy.stats.randint(10, 100)\n",
    "    }\n",
    "    dt_cv = sklearn.model_selection.RandomizedSearchCV(dt, param_distributions=dt_random_params, n_iter=n_iter_num,\n",
    "                                                       cv=cv_num,\n",
    "                                                       scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42,\n",
    "                                                       verbose=4)\n",
    "    dt_cv.fit(X_train, y_train)\n",
    "    return dt_cv\n",
    "\n",
    "\n",
    "def train_RandomForest(X_train, y_train):\n",
    "    print('Training RandomForest ...')\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]\n",
    "    features = ['auto', 'sqrt']\n",
    "    depth = [int(x) for x in np.linspace(5, 30, num=6)]\n",
    "    samples_split = [2, 5, 10, 15, 100]\n",
    "    sample_leaf = [1, 2, 5, 10]\n",
    "\n",
    "    rf_random_params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': features,\n",
    "        'max_depth': depth,\n",
    "        'min_samples_split': samples_split,\n",
    "        'min_samples_leaf': sample_leaf\n",
    "    }\n",
    "    rf_cv = sklearn.model_selection.RandomizedSearchCV(rf, param_distributions=rf_random_params, n_iter=n_iter_num,\n",
    "                                                       cv=cv_num,\n",
    "                                                       scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42,\n",
    "                                                       verbose=4)\n",
    "    rf_cv.fit(X_train, y_train)\n",
    "    return rf_cv\n",
    "\n",
    "\n",
    "def train_AdaBoost(X_train, y_train):\n",
    "    print('Training AdaBoost ...')\n",
    "    ada_boost = AdaBoostRegressor(random_state=0)\n",
    "    ada_boost_random_params = {\n",
    "        'loss': ['linear', 'square', 'exponential'],\n",
    "        'learning_rate': scipy.stats.uniform(0.75, 1.25),\n",
    "        'n_estimators': scipy.stats.randint(40, 100)\n",
    "    }\n",
    "    ada_boost_cv = sklearn.model_selection.RandomizedSearchCV(ada_boost, param_distributions=ada_boost_random_params,\n",
    "                                                              n_iter=n_iter_num, cv=cv_num,\n",
    "                                                              scoring='neg_mean_absolute_error', n_jobs=-1,\n",
    "                                                              random_state=42, verbose=4)\n",
    "    ada_boost_cv.fit(X_train, y_train)\n",
    "    return ada_boost_cv\n",
    "\n",
    "\n",
    "def train_GaussianProcess(X_train, y_train):\n",
    "    print('Training GaussianProcess ...')\n",
    "    alpha = 1e-9\n",
    "    gaussian = GaussianProcessRegressor(normalize_y=True, random_state=0, optimizer=None, alpha=alpha)\n",
    "    gaussian_random_params = {\n",
    "        'kernel': [DotProduct(), WhiteKernel(), RBF(), Matern(), RationalQuadratic()],\n",
    "        'n_restarts_optimizer': scipy.stats.randint(0, 10),\n",
    "        #         'alpha' : scipy.stats.uniform(1e-9, 1e-8)\n",
    "    }\n",
    "    gaussian_cv = sklearn.model_selection.RandomizedSearchCV(gaussian, param_distributions=gaussian_random_params,\n",
    "                                                             n_iter=n_iter_num, cv=cv_num,\n",
    "                                                             scoring='neg_mean_absolute_error', n_jobs=-1,\n",
    "                                                             random_state=42, verbose=4)\n",
    "    gaussian_cv.fit(X_train, y_train)\n",
    "    return gaussian_cv\n",
    "\n",
    "\n",
    "def run_all_regrs(X_train, y_train):\n",
    "    all_regrs = []\n",
    "    regr_names = []\n",
    "\n",
    "    svm = train_SVR(X_train, y_train)\n",
    "    all_regrs.append(svm.best_estimator_)\n",
    "    regr_names.append('SVR')\n",
    "\n",
    "    decision_tree = train_DecisionTree(X_train, y_train)\n",
    "    all_regrs.append(decision_tree.best_estimator_)\n",
    "    regr_names.append('Decision Tree')\n",
    "\n",
    "    random_forest = train_RandomForest(X_train, y_train)\n",
    "    all_regrs.append(random_forest.best_estimator_)\n",
    "    regr_names.append('Random Forest')\n",
    "\n",
    "    ada_boost = train_AdaBoost(X_train, y_train)\n",
    "    all_regrs.append(ada_boost.best_estimator_)\n",
    "    regr_names.append('AdaBoost')\n",
    "\n",
    "    gaussian_process = train_GaussianProcess(X_train, y_train)\n",
    "    all_regrs.append(gaussian_process.best_estimator_)\n",
    "    regr_names.append('Gaussian Process')\n",
    "\n",
    "    linear_regression = train_LinearRegression(X_train, y_train)\n",
    "    all_regrs.append(linear_regression.best_estimator_)\n",
    "    regr_names.append('Linear Regression')\n",
    "\n",
    "    print(all_regrs)\n",
    "    print(regr_names)\n",
    "\n",
    "    return svm, decision_tree, random_forest, ada_boost, gaussian_process, linear_regression\n",
    "\n",
    "\n",
    "def model_evaluation(models, X_train, y_train, X_test, y_test, X_df, y_df, cv_num):\n",
    "    cross_val = []\n",
    "    r2_train = []\n",
    "    r2_test = []\n",
    "    mae_score = []\n",
    "    rmse_score = []\n",
    "    best_estimator = []\n",
    "    regr_names = []\n",
    "\n",
    "    for model in models:\n",
    "        cv = cross_val_score(estimator=model, X=X_df, y=y_df, cv=cv_num)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        r2_score_train = r2_score(y_train, y_pred_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        r2_score_test = r2_score(y_test, y_pred_test)\n",
    "        rmse = (np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "        mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        cross_val.append(cv.mean())\n",
    "        r2_train.append(r2_score_train)\n",
    "        r2_test.append(r2_score_test)\n",
    "        mae_score.append(mae)\n",
    "        rmse_score.append(rmse)\n",
    "        best_estimator.append(model.best_estimator_)\n",
    "        regr_names.append(model)\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        list(zip(cross_val, r2_train, r2_test, mae_score, rmse_score, best_estimator, regr_names)),\n",
    "        columns=['Cross Validation Score', 'R^2 Train', 'R^2 Test', 'MAE', 'RMSE', 'Best estimator',\n",
    "                 'Regression Model'])\n",
    "    return metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}